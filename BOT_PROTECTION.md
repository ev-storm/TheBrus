# Защита от ботов - Инструкция по настройке

## Что было настроено:

### 1. Файл robots.txt

- Запрещен доступ ко всем ботам (`Disallow: /`)
- Установлена задержка между запросами (Crawl-Delay: 86400)
- Закомментированы правила для основных поисковых систем (можно раскомментировать при необходимости)

### 2. Мета-теги в HTML

Добавлены в `nuxt.config.ts`:

- `noindex` - запрет индексации
- `nofollow` - запрет перехода по ссылкам
- `noarchive` - запрет кэширования
- `nosnippet` - запрет отображения сниппетов
- `noimageindex` - запрет индексации изображений

### 3. Middleware для Nuxt

Создан `middleware/block-bots.global.ts` который:

- Проверяет User-Agent запросов
- Блокирует известных ботов
- Возвращает 403 Forbidden для заблокированных запросов

### 4. .htaccess для Apache

Создан файл `public/.htaccess` с правилами:

- Блокировка по User-Agent
- Запрет доступа к служебным файлам
- Дополнительные заголовки безопасности

## Дополнительные рекомендации:

### Для Nginx серверов

Добавьте в конфигурацию nginx:

```nginx
# Блокировка ботов
if ($http_user_agent ~* (bot|crawler|spider|scraper|scanner|harvester|extractor)) {
    return 403;
}

# Блокировка известных ботов
if ($http_user_agent ~* (Googlebot|Bingbot|YandexBot|Slurp|DuckDuckBot|Baiduspider)) {
    return 403;
}

# Заголовки для запрета индексации
add_header X-Robots-Tag "noindex, nofollow, noarchive, nosnippet, noimageindex" always;
```

### Для Cloudflare

1. Перейдите в Security → WAF
2. Создайте правило для блокировки ботов по User-Agent
3. Добавьте правило для блокировки по странам (если нужно)

### Мониторинг

Рекомендуется настроить мониторинг:

- Логирование заблокированных запросов
- Анализ трафика на предмет подозрительной активности
- Регулярная проверка эффективности защиты

## Важные замечания:

- Некоторые легитимные боты (Google, Yandex) могут быть заблокированы
- При необходимости раскомментируйте соответствующие строки в robots.txt
- Регулярно обновляйте список заблокированных User-Agent
- Тестируйте сайт после внесения изменений
